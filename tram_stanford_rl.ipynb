{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/oektomo/KelasRLG2/blob/master/tram_stanford_rl.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "bfda1805",
      "metadata": {
        "id": "bfda1805"
      },
      "outputs": [],
      "source": [
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "afddc655",
      "metadata": {
        "id": "afddc655"
      },
      "outputs": [],
      "source": [
        "### Model (MDP problem)\n",
        "\n",
        "class TransportationMDP(object):\n",
        "    def __init__(self, N):\n",
        "        # N = number of blocks\n",
        "        self.N = N\n",
        "    def startState(self):\n",
        "        return 1\n",
        "    def isEnd(self, state):\n",
        "        return state == self.N\n",
        "    def actions(self, state):\n",
        "        # return list of valid actions\n",
        "        result = []\n",
        "        if state+1<=self.N:\n",
        "            result.append('walk')\n",
        "        if state*2<=self.N:\n",
        "            result.append('tram')\n",
        "        return result\n",
        "    def succProbReward(self, state, action):\n",
        "        # return list of (newState, prob, reward) triples\n",
        "        # state = s, action = a, newState = s'\n",
        "        # prob = T(s, a, s'), reward = Reward(s, a, s')\n",
        "        result = []\n",
        "        if action=='walk':\n",
        "            result.append((state+1, 1., -1.))\n",
        "        elif action=='tram':\n",
        "            failProb = 0\n",
        "            result.append((state*2, 1.-failProb, -2.))\n",
        "            result.append((state, failProb, -2.))\n",
        "        return result\n",
        "    def discount(self):\n",
        "        return 1.\n",
        "    def states(self):\n",
        "        return range(1, self.N+1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "3a7c031f",
      "metadata": {
        "id": "3a7c031f"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Inference (Algorithms)\n",
        "\n",
        "def valueIteration(mdp):\n",
        "    # initialize\n",
        "    V = {} # state -> Vopt[state]\n",
        "    for state in mdp.states():\n",
        "        V[state] = 0.\n",
        "\n",
        "    def Q(state, action):\n",
        "        return sum(prob*(reward + mdp.discount()*V[newState]) \\\n",
        "                for newState, prob, reward in mdp.succProbReward(state, action))\n",
        "\n",
        "    while True:\n",
        "        # compute the new values (newV) given the old values (V)\n",
        "        newV = {}\n",
        "        for state in mdp.states():\n",
        "            if mdp.isEnd(state):\n",
        "                newV[state] = 0.\n",
        "            else:\n",
        "                newV[state] = max(Q(state, action) for action in mdp.actions(state))\n",
        "        # check for convergence\n",
        "        if max(abs(V[state]-newV[state]) for state in mdp.states())<1e-10:\n",
        "            break\n",
        "        V = newV\n",
        "\n",
        "        # read out policy\n",
        "        pi = {}\n",
        "        for state in mdp.states():\n",
        "            if mdp.isEnd(state):\n",
        "                pi[state] = 'none'\n",
        "            else:\n",
        "                pi[state] = max((Q(state, action), action) for action in mdp.actions(state))[1]\n",
        "\n",
        "        # print stuff out\n",
        "        os.system('clear')\n",
        "        print('{:20} {:20} {:20}'.format('s', 'V(s)', 'pi(s)'))\n",
        "        for state in mdp.states():\n",
        "            print('{:20} {:20} {:20}'.format(state, V[state], pi[state]))\n",
        "        input()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "c57b8cc4",
      "metadata": {
        "id": "c57b8cc4",
        "outputId": "2721e683-e7e7-46ae-a96b-c26abdc8fa88",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['walk', 'tram']\n",
            "[(4, 1.0, -1.0)]\n",
            "[(6, 1.0, -2.0), (3, 0, -2.0)]\n"
          ]
        }
      ],
      "source": [
        "mdp = TransportationMDP(N=30)\n",
        "print(mdp.actions(3))\n",
        "print(mdp.succProbReward(3, 'walk'))\n",
        "print(mdp.succProbReward(3, 'tram'))\n",
        "#valueIteration(mdp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5514baba",
      "metadata": {
        "id": "5514baba"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4d2e8ce",
      "metadata": {
        "id": "c4d2e8ce"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "name": "tram_stanford_rl.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}